@article{devlin2018bert,
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  title   = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year    = {2018}
}

@misc{naveed2023comprehensive,
  archiveprefix = {arXiv},
  author        = {Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
  eprint        = {2307.06435},
  primaryclass  = {cs.CL},
  title         = {A Comprehensive Overview of Large Language Models},
  year          = {2023}
}

@misc{zhou2023comprehensive,
  archiveprefix = {arXiv},
  author        = {Ce Zhou and Qian Li and Chen Li and Jun Yu and Yixin Liu and Guangjing Wang and Kai Zhang and Cheng Ji and Qiben Yan and Lifang He and Hao Peng and Jianxin Li and Jia Wu and Ziwei Liu and Pengtao Xie and Caiming Xiong and Jian Pei and Philip S. Yu and Lichao Sun},
  eprint        = {2302.09419},
  primaryclass  = {cs.AI},
  title         = {A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT},
  year          = {2023}
}

@misc{yang2023harnessing,
  archiveprefix = {arXiv},
  author        = {Jingfeng Yang and Hongye Jin and Ruixiang Tang and Xiaotian Han and Qizhang Feng and Haoming Jiang and Bing Yin and Xia Hu},
  eprint        = {2304.13712},
  primaryclass  = {cs.CL},
  title         = {Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond},
  year          = {2023}
}