@inproceedings{10.1145/3531146.3533088,
  abstract  = {Responsible innovation on large-scale Language Models (LMs) requires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxonomy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from computer science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessments of other language technologies, and situate these in the same taxonomy. We underscore that it is the responsibility of organizations to engage with the mitigations we discuss throughout the paper. We close by highlighting challenges and directions for further research on risk evaluation and mitigation with the goal of ensuring that language models are developed responsibly.},
  address   = {New York, NY, USA},
  author    = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
  booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  doi       = {10.1145/3531146.3533088},
  isbn      = {9781450393522},
  keywords  = {language models, risk assessment, responsible AI, technology risks, responsible innovation},
  location  = {Seoul, Republic of Korea},
  numpages  = {16},
  pages     = {214–229},
  publisher = {Association for Computing Machinery},
  series    = {FAccT '22},
  title     = {Taxonomy of Risks Posed by Language Models},
  url       = {https://doi.org/10.1145/3531146.3533088},
  year      = {2022}
}

@misc{anil2023palm,
  archiveprefix = {arXiv},
  author        = {Rohan Anil and Andrew M. Dai and Orhan Firat and Melvin Johnson and Dmitry Lepikhin and Alexandre Passos and Siamak Shakeri and Emanuel Taropa and Paige Bailey and Zhifeng Chen and Eric Chu and Jonathan H. Clark and Laurent El Shafey and Yanping Huang and Kathy Meier-Hellstern and Gaurav Mishra and Erica Moreira and Mark Omernick and Kevin Robinson and Sebastian Ruder and Yi Tay and Kefan Xiao and Yuanzhong Xu and Yujing Zhang and Gustavo Hernandez Abrego and Junwhan Ahn and Jacob Austin and Paul Barham and Jan Botha and James Bradbury and Siddhartha Brahma and Kevin Brooks and Michele Catasta and Yong Cheng and Colin Cherry and Christopher A. Choquette-Choo and Aakanksha Chowdhery and Clément Crepy and Shachi Dave and Mostafa Dehghani and Sunipa Dev and Jacob Devlin and Mark Díaz and Nan Du and Ethan Dyer and Vlad Feinberg and Fangxiaoyu Feng and Vlad Fienber and Markus Freitag and Xavier Garcia and Sebastian Gehrmann and Lucas Gonzalez and Guy Gur-Ari and Steven Hand and Hadi Hashemi and Le Hou and Joshua Howland and Andrea Hu and Jeffrey Hui and Jeremy Hurwitz and Michael Isard and Abe Ittycheriah and Matthew Jagielski and Wenhao Jia and Kathleen Kenealy and Maxim Krikun and Sneha Kudugunta and Chang Lan and Katherine Lee and Benjamin Lee and Eric Li and Music Li and Wei Li and YaGuang Li and Jian Li and Hyeontaek Lim and Hanzhao Lin and Zhongtao Liu and Frederick Liu and Marcello Maggioni and Aroma Mahendru and Joshua Maynez and Vedant Misra and Maysam Moussalem and Zachary Nado and John Nham and Eric Ni and Andrew Nystrom and Alicia Parrish and Marie Pellat and Martin Polacek and Alex Polozov and Reiner Pope and Siyuan Qiao and Emily Reif and Bryan Richter and Parker Riley and Alex Castro Ros and Aurko Roy and Brennan Saeta and Rajkumar Samuel and Renee Shelby and Ambrose Slone and Daniel Smilkov and David R. So and Daniel Sohn and Simon Tokumine and Dasha Valter and Vijay Vasudevan and Kiran Vodrahalli and Xuezhi Wang and Pidong Wang and Zirui Wang and Tao Wang and John Wieting and Yuhuai Wu and Kelvin Xu and Yunhan Xu and Linting Xue and Pengcheng Yin and Jiahui Yu and Qiao Zhang and Steven Zheng and Ce Zheng and Weikang Zhou and Denny Zhou and Slav Petrov and Yonghui Wu},
  eprint        = {2305.10403},
  primaryclass  = {cs.CL},
  title         = {PaLM 2 Technical Report},
  year          = {2023}
}

@online{ChatGPT-announcement,
  author = {OpenAI},
  date   = {30.11.2022},
  title  = {Introducing ChatGPT},
  url    = {https://openai.com/blog/chatgpt},
  year   = {2022}
}

@article{devlin2018bert,
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  title   = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year    = {2018}
}

@online{google_trends_ki,
  author = {Google},
  note   = {Accessed: \today},
  title  = {Google Trends},
  url    = {https://trends.google.com/trends/explore?date=2021-01-11%202023-12-07&q=%2Fm%2F0mkz&hl=de-DE}
}

@online{google:llm,
  author  = {Google},
  date    = {08.08.2023},
  title   = {Einführung in große Sprachmodelle},
  url     = {https://developers.google.com/machine-learning/resources/intro-llms?hl=de},
  urldate = {10.12.2023},
  year    = {2023}
}

@article{llm_comparison,
  author = {Guo, Tong},
  doi    = {10.36227/techrxiv.14820348.v1},
  month  = {06},
  pages  = {},
  title  = {A Comprehensive Comparison of Pre-training Language Models},
  year   = {2021}
}



@misc{naveed2023comprehensive,
  archiveprefix = {arXiv},
  author        = {Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
  eprint        = {2307.06435},
  primaryclass  = {cs.CL},
  title         = {A Comprehensive Overview of Large Language Models},
  year          = {2023}
}

@book{NLP_2008,
  author = {Jurafsky, Daniel and Martin, James},
  month  = {02},
  pages  = {},
  title  = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  volume = {2},
  year   = {2008}
}


@misc{tamkin2021understanding,
  archiveprefix = {arXiv},
  author        = {Alex Tamkin and Miles Brundage and Jack Clark and Deep Ganguli},
  eprint        = {2102.02503},
  primaryclass  = {cs.CL},
  title         = {Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models},
  year          = {2021}
}



@inbook{Taulli2023,
  abstract  = {In December 2015, a group of tech veterans -- including Elon Musk, Reid Hoffman, Peter Thiel, and Sam Altman -- founded OpenAI. They pledged over {\$}1 billion for the venture.},
  address   = {Berkeley, CA},
  author    = {Taulli, Tom},
  booktitle = {Generative AI: How ChatGPT and Other AI Tools Will Revolutionize Business},
  doi       = {10.1007/978-1-4842-9367-6_5},
  isbn      = {978-1-4842-9367-6},
  pages     = {93--125},
  publisher = {Apress},
  title     = {Large Language Models},
  url       = {https://doi.org/10.1007/978-1-4842-9367-6_5},
  year      = {2023}
}

@article{Tokayev_2023,
  abstractnote = {&amp;lt;p&amp;gt;Large Language Models (LLMs) have become increasingly prevalent in various sectors including healthcare, finance, and customer service, among others. While these models offer impressive capabilities ranging from natural language understanding to text generation, their widespread adoption has raised a series of ethical concerns. This research aims to provide an in-depth analysis of these ethical implications, organized into several categories for better understanding. On the societal front, LLMs can amplify existing biases found in their training data, contributing to unfair or harmful outputs. Additionally, these models can be employed to generate fake news or misleading information, undermining public trust and contributing to social discord. There is also the risk of cultural homogenization as these technologies may promote dominant cultures at the expense of local or minority perspectives. From an economic and environmental standpoint, the energy-intensive process of training LLMs results in a significant carbon footprint, raising sustainability concerns. The advent of LLMs also presents economic challenges, particularly the potential displacement of jobs due to automation, exacerbating employment insecurity. On the operational level, LLMs pose technical challenges such as a lack of transparency, often referred to as the &amp;quot;black box&amp;quot; nature of these models, making it difficult to understand or rectify their behavior. This opacity can lead to an over-reliance on LLMs for critical decision-making, without adequate scrutiny or understanding of their limitations. Further, there are significant privacy concerns, as these models may inadvertently generate outputs containing sensitive or confidential information gleaned from their training data. The human experience is also affected, as reliance on LLMs for various tasks can lead to depersonalization of human interactions. Finally, questions surrounding access, equity, and governance of these technologies come to the forefront. Control and accountability remain nebulous, especially when LLMs are used for critical decision-making or actions that have direct human impact. Moreover, the access to such advanced technologies may be limited to well-resourced entities, widening existing inequalities. This research seeks to delve into these issues, aiming to spark informed discussions and guide future policy.&amp;lt;/p&amp;gt;},
  author       = {Tokayev, Kassym-Jomart},
  journal      = {International Journal of Social Analytics},
  month        = {Sep.},
  number       = {9},
  pages        = {17–33},
  title        = {Ethical Implications of Large Language Models A Multidimensional Exploration of Societal, Economic, and Technical Concerns},
  url          = {https://norislab.com/index.php/ijsa/article/view/42},
  volume       = {8},
  year         = {2023}
}

@misc{touvron2023llama,
  archiveprefix = {arXiv},
  author        = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  eprint        = {2302.13971},
  primaryclass  = {cs.CL},
  title         = {LLaMA: Open and Efficient Foundation Language Models},
  year          = {2023}
}


@misc{yang2023harnessing,
  archiveprefix = {arXiv},
  author        = {Jingfeng Yang and Hongye Jin and Ruixiang Tang and Xiaotian Han and Qizhang Feng and Haoming Jiang and Bing Yin and Xia Hu},
  eprint        = {2304.13712},
  primaryclass  = {cs.CL},
  title         = {Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond},
  year          = {2023}
}

@inproceedings{zellers2019hellaswag,
  author    = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  title     = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  year      = {2019}
}

@misc{zhou2023comprehensive,
  archiveprefix = {arXiv},
  author        = {Ce Zhou and Qian Li and Chen Li and Jun Yu and Yixin Liu and Guangjing Wang and Kai Zhang and Cheng Ji and Qiben Yan and Lifang He and Hao Peng and Jianxin Li and Jia Wu and Ziwei Liu and Pengtao Xie and Caiming Xiong and Jian Pei and Philip S. Yu and Lichao Sun},
  eprint        = {2302.09419},
  primaryclass  = {cs.AI},
  title         = {A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT},
  year          = {2023}
}

@article{Peng_2023,
  author    = {Peng, Cheng and Yang, Xi and Chen, Aokun and Smith, Kaleb E. and PourNejatian, Nima and Costa, Anthony B. and Martin, Cheryl and Flores, Mona G. and Zhang, Ying and Magoc, Tanja and Lipori, Gloria and Mitchell, Duane A. and Ospina, Naykky S. and Ahmed, Mustafa M. and Hogan, William R. and Shenkman, Elizabeth A. and Guo, Yi and Bian, Jiang and Wu, Yonghui},
  doi       = {10.1038/s41746-023-00958-w},
  issn      = {2398-6352},
  journal   = {npj Digital Medicine},
  month     = nov,
  number    = {1},
  publisher = {Springer Science and Business Media LLC},
  title     = {A study of generative large language model for medical research and healthcare},
  url       = {http://dx.doi.org/10.1038/s41746-023-00958-w},
  volume    = {6},
  year      = {2023}
}

@misc{yetiştiren2023evaluating,
  archiveprefix = {arXiv},
  author        = {Burak Yetiştiren and Işık Özsoy and Miray Ayerdem and Eray Tüzün},
  eprint        = {2304.10778},
  primaryclass  = {cs.SE},
  title         = {Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT},
  year          = {2023}
}

@misc{vaswani2023attention,
  archiveprefix = {arXiv},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  eprint        = {1706.03762},
  primaryclass  = {cs.CL},
  title         = {Attention Is All You Need},
  year          = {2023}
}

@misc{lv2023parameter,
  archiveprefix = {arXiv},
  author        = {Kai Lv and Yuqing Yang and Tengxiao Liu and Qinghui Gao and Qipeng Guo and Xipeng Qiu},
  eprint        = {2306.09782},
  primaryclass  = {cs.CL},
  title         = {Full Parameter Fine-tuning for Large Language Models with Limited Resources},
  year          = {2023}
}