@article{devlin2018bert,
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  title   = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year    = {2018}
}

@misc{naveed2023comprehensive,
  archiveprefix = {arXiv},
  author        = {Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
  eprint        = {2307.06435},
  primaryclass  = {cs.CL},
  title         = {A Comprehensive Overview of Large Language Models},
  year          = {2023}
}

@misc{zhou2023comprehensive,
  archiveprefix = {arXiv},
  author        = {Ce Zhou and Qian Li and Chen Li and Jun Yu and Yixin Liu and Guangjing Wang and Kai Zhang and Cheng Ji and Qiben Yan and Lifang He and Hao Peng and Jianxin Li and Jia Wu and Ziwei Liu and Pengtao Xie and Caiming Xiong and Jian Pei and Philip S. Yu and Lichao Sun},
  eprint        = {2302.09419},
  primaryclass  = {cs.AI},
  title         = {A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT},
  year          = {2023}
}

@misc{yang2023harnessing,
  archiveprefix = {arXiv},
  author        = {Jingfeng Yang and Hongye Jin and Ruixiang Tang and Xiaotian Han and Qizhang Feng and Haoming Jiang and Bing Yin and Xia Hu},
  eprint        = {2304.13712},
  primaryclass  = {cs.CL},
  title         = {Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond},
  year          = {2023}
}

@article{Tokayev_2023,
  abstractnote = {&amp;lt;p&amp;gt;Large Language Models (LLMs) have become increasingly prevalent in various sectors including healthcare, finance, and customer service, among others. While these models offer impressive capabilities ranging from natural language understanding to text generation, their widespread adoption has raised a series of ethical concerns. This research aims to provide an in-depth analysis of these ethical implications, organized into several categories for better understanding. On the societal front, LLMs can amplify existing biases found in their training data, contributing to unfair or harmful outputs. Additionally, these models can be employed to generate fake news or misleading information, undermining public trust and contributing to social discord. There is also the risk of cultural homogenization as these technologies may promote dominant cultures at the expense of local or minority perspectives. From an economic and environmental standpoint, the energy-intensive process of training LLMs results in a significant carbon footprint, raising sustainability concerns. The advent of LLMs also presents economic challenges, particularly the potential displacement of jobs due to automation, exacerbating employment insecurity. On the operational level, LLMs pose technical challenges such as a lack of transparency, often referred to as the &amp;quot;black box&amp;quot; nature of these models, making it difficult to understand or rectify their behavior. This opacity can lead to an over-reliance on LLMs for critical decision-making, without adequate scrutiny or understanding of their limitations. Further, there are significant privacy concerns, as these models may inadvertently generate outputs containing sensitive or confidential information gleaned from their training data. The human experience is also affected, as reliance on LLMs for various tasks can lead to depersonalization of human interactions. Finally, questions surrounding access, equity, and governance of these technologies come to the forefront. Control and accountability remain nebulous, especially when LLMs are used for critical decision-making or actions that have direct human impact. Moreover, the access to such advanced technologies may be limited to well-resourced entities, widening existing inequalities. This research seeks to delve into these issues, aiming to spark informed discussions and guide future policy.&amp;lt;/p&amp;gt;},
  author       = {Tokayev, Kassym-Jomart},
  journal      = {International Journal of Social Analytics},
  month        = {Sep.},
  number       = {9},
  pages        = {17–33},
  title        = {Ethical Implications of Large Language Models A Multidimensional Exploration of Societal, Economic, and Technical Concerns},
  url          = {https://norislab.com/index.php/ijsa/article/view/42},
  volume       = {8},
  year         = {2023}
}

@inproceedings{10.1145/3531146.3533088,
  abstract  = {Responsible innovation on large-scale Language Models (LMs) requires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxonomy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from computer science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessments of other language technologies, and situate these in the same taxonomy. We underscore that it is the responsibility of organizations to engage with the mitigations we discuss throughout the paper. We close by highlighting challenges and directions for further research on risk evaluation and mitigation with the goal of ensuring that language models are developed responsibly.},
  address   = {New York, NY, USA},
  author    = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
  booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  doi       = {10.1145/3531146.3533088},
  isbn      = {9781450393522},
  keywords  = {language models, risk assessment, responsible AI, technology risks, responsible innovation},
  location  = {Seoul, Republic of Korea},
  numpages  = {16},
  pages     = {214–229},
  publisher = {Association for Computing Machinery},
  series    = {FAccT '22},
  title     = {Taxonomy of Risks Posed by Language Models},
  url       = {https://doi.org/10.1145/3531146.3533088},
  year      = {2022}
}

@misc{tamkin2021understanding,
  archiveprefix = {arXiv},
  author        = {Alex Tamkin and Miles Brundage and Jack Clark and Deep Ganguli},
  eprint        = {2102.02503},
  primaryclass  = {cs.CL},
  title         = {Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models},
  year          = {2021}
}
