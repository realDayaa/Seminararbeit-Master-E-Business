\section{Aktuelle Ranking} \label{sec:aktuelle_rankings}
Abschließend ist es wichtig, das Ranking von \acp{llm} zu betrachten, das ein zentrales Element in der Bewertung und dem Vergleich dieser Modelle darstellt. Plattformen wie Hugging Face haben hierfür spezielle Leaderboards eingerichtet, auf denen Modelle anhand ihrer Leistung in verschiedenen Benchmarks verglichen werden können. Diese Ranglisten bieten eine klare und leicht zugängliche Übersicht darüber, wie verschiedene Modelle in standardisierten Tests abschneiden.

Die Leaderboards von Hugging Face und ähnlichen Plattformen sind insofern wertvoll, als sie eine transparente Basis für den Vergleich der Leistungsfähigkeit verschiedener \acp{llm} bieten. Sie ermöglichen es Nutzern, Modelle anhand objektiver Kriterien zu beurteilen und somit eine informierte Auswahl für ihre spezifischen Anforderungen zu treffen. Dies ist besonders nützlich in einem Feld, das so schnelllebig und divers ist wie das der Sprachmodelle.
Das Open LLM Leaderboard \footcite[Vgl.][]{open-llm-leaderboard} von Huggingface beispielsweise nutzt unter anderem den in Kapitel \ref{sec:exbenchmarks} angesprochenen Winograde Benchmark.
Das besondere an dieser Rangliste ist, dass nur frei verfügbare Modelle aufgeführt werden, die von der Community zur Verfügung gestellt werden.
Dies ermöglicht es, die Leistungsfähigkeit von Modellen zu vergleichen, die von verschiedenen Entwicklern und Forschern erstellt wurden, und bietet so eine umfassende Übersicht über die aktuelle Entwicklung im Bereich der \acp{llm}.
Closed Source Modelle wie GPT-4 von OpenAI \footcite[Vgl.][S. 1]{openai2023gpt4} sind daher hier nicht aufgeführt, da sie nicht öffentlich zugänglich sind.

Allerdings sollten die Rankings auf diesen Leaderboards mit Bedacht interpretiert werden. Wie zuvor in Kapitel \ref{sec:problematiken} diskutiert, können Benchmarks bestimmte Einschränkungen und Verzerrungen aufweisen.
Daher ist es möglich, dass ein hochrangiges Modell in einem Benchmark nicht unbedingt in allen realen Anwendungsszenarien überlegen ist.
Es ist wichtig, die spezifischen Kriterien und Bedingungen, unter denen die Tests durchgeführt wurden, zu verstehen, um eine vollständige Einschätzung der Modellleistung zu erhalten.
Offene Ranglisten wie das Open LLM Leaderboard bieten hierfür eine gute Grundlage, da sie die verwendeten Benchmarks und die zugrunde liegenden Datenquellen transparent machen und alle Modelle unter den gleichen Voraussetzungen testen.

Darüber hinaus reflektieren die Rankings auf diesen Leaderboards oft nur die Leistung in spezifischen Aufgaben und berücksichtigen möglicherweise nicht die allgemeine Anwendbarkeit oder Effizienz der Modelle.
Bei der Auswahl eines Modells für praktische Anwendungen sollten daher neben den Benchmark-Ergebnissen auch Faktoren wie Ressourcenbedarf, Benutzerfreundlichkeit und die Anpassungsfähigkeit an spezifische Anforderungen berücksichtigt werden.
