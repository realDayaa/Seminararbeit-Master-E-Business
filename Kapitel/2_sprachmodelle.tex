\section{Sprachmodelle und ihre Bedeutung} \label{sec:sprachmodelle}
\subsection{Sprachmodelle allgemein} \label{sec:sprachmodelle_allgemein}
\acp{llm} sind hochentwickelte \ac{ki}-Systeme, die darauf spezialisiert sind, menschenähnlichen Text zu verarbeiten, zu verstehen und zu generieren.
Sie basieren auf fortschrittlichen Deep-Learning-Techniken und werden durch umfangreiche Datensätze trainiert, die Milliarden von Wörtern aus verschiedenen Quellen wie Websites, Büchern und Artikeln enthalten.
Durch dieses umfangreiche Training können \acp{llm} die Besonderheiten von Sprache, Grammatik, Kontext und sogar einige Aspekte des Allgemeinwissens erfassen. \footcite[Vgl.][S. 93 ff.]{Taulli2023}

Sie sind insofern eine Weiterentwicklung des \ac{nlp}, als sie nicht nur einzelne Wörter oder Sätze verarbeiten \footcite[Vgl.][S. 245 ff.]{NLP_2008}, sondern auch die Beziehungen zwischen Wörtern und Sätzen verstehen können.
Dies ermöglicht es ihnen, Texte zu analysieren und zu verstehen, um dann auf dieser Grundlage neue Texte zu generieren.
Das tun sie, indem sie die Wahrscheinlichkeit berechnen, mit der ein bestimmtes Wort in einem Satz oder Text erscheint, dem sogenannten \textit{Token}. \footcite[Vgl.][]{google:llm}

Trotz der kontinuierlichen Weiterentwicklung von \acp{llm} und ihres erheblichen Potenzials zur Verbesserung und Automatisierung verschiedener Anwendungen in Branchen wie Kundenservice, Inhaltserstellung, Bildung und Forschung \footcite[Vgl.][S. 4 ff.]{tamkin2021understanding}, müssen auch ethische und gesellschaftliche Bedenken berücksichtigt werden. Dazu gehören mögliche voreingenommene Verhaltensweisen und Missbrauch, die im Rahmen des technologischen Fortschritts aktiv angegangen werden müssen \footcite[Vgl.][S. 1]{Tokayev_2023}.
Weiterhin ist es wichtig sich im Klaren zu sein, dass Informationen die von \acp{llm} generiert werden, nicht unbedingt der Wahrheit entsprechen müssen \footcite[Vgl.][S. 217 f.]{10.1145/3531146.3533088}, da sie lediglich auf der Wahrscheinlichkeit basieren, mit der ein bestimmtes Wort in einem Satz oder Text erscheint.

\subsection{Einsatzgebiete} \label{sec:einsatzgebiete}
LM-Technologien finden in verschiedenen Anwendungsfeldern vielfältige Verwendungsmöglichkeiten. Yang u.a. haben sechs unterschiedliche Anwendungsszenarien identifiziert \footcite[Vgl.][S. 6 ff.]{yang2023harnessing}, darunter insbesondere solche im Bereich des \ac{nlu}. Hierzu zählen beispielsweise Textklassifikationen, die einen Schwerpunkt auf die semantische Erfassung von natürlicher Sprache legen. Ebenfalls relevant sind Generierungsaufgaben, die sich auf die automatisierte Erzeugung von sprachlichen Inhalten beziehen. Des Weiteren lassen sich wissensintensive Aufgaben als bedeutsames Anwendungsfeld konstatieren, bei dem die Modelle dazu dienen, umfangreiche Wissensbestände zu verarbeiten und zu extrahieren.
Die Vielfalt an Anwendungsfeldern der Sprachmodelle ermöglicht ihren Einsatz in unterschiedlichsten Kontexten \footcite[Vgl.][S. 6 ff.]{yang2023harnessing}.
So können sie beispielsweise in der Medizin eingesetzt werden, um die Diagnose von Krankheiten zu unterstützen \footcite[Vgl.][S. 10 ff.]{Peng_2023}.
Ein weiteres Anwendungsfeld ist auch die Unterstützung in der Erstellung von Programmcode \footcite[Vgl.][S. 3 ff.]{yetiştiren2023evaluating}.
Dabei basieren unterschiedliche Lösungen wie Github Copilot oder Amazon CodeWhisperer auf verschiedenen Sprachmodellen.

\subsection{Abgrenzung verschiedener Modelle Untereinander} \label{sec:abgrenzung}
Unterschiede zwischen den verschiedenen \acp{llm} können anhand verschiedener Kriterien festgemacht werden.
Als Erstes sind hier die verwendete Architektur zu nennen.
So basierten \acp{llm} vor der Einführung der Transformern im Jahr 2017 \footcite[Vgl.][S. 1]{vaswani2023attention} hauptsächlich auf rekurrenten neuronalen Netzwerken (RNNs) und Long Short-Term Memory (LSTM) Modellen.
Diese Modelle waren in der Lage, sequentielle Daten zu verarbeiten und dabei den Kontext zu berücksichtigen, hatten jedoch Schwierigkeiten mit langen Abhängigkeiten und waren rechenintensiv.
Mit der Einführung der Transformer-Architektur wurde ein Paradigmenwechsel in der Modellierung von Sprache erreicht. Transformer nutzen Selbst-Attention-Mechanismen, um Beziehungen zwischen Wörtern in einem Text unabhängig von ihrer Position zu erfassen. Dies ermöglicht es ihnen, Kontext über größere Distanzen hinweg zu erfassen und effizienter zu trainieren.
Ein weiteres Unterscheidungsmerkmal ist die Art und Weise, wie die Modelle trainiert werden. Einige Modelle werden auf spezifischen Aufgaben trainiert, während andere in einem unsupervised oder semi-supervised Setting trainiert werden, wobei sie versuchen, die Wahrscheinlichkeit des nächsten Wortes in einem Text zu maximieren.
Schließlich können \acp{llm} auch anhand der Größe der Modelle unterschieden werden, gemessen an der Anzahl der Parameter.
Größere Modelle haben in der Regel eine höhere Kapazität und können komplexere Muster erfassen, benötigen jedoch mehr Rechenleistung und Speicherplatz.
Kleinere Modelle sind effizienter, können aber unter Umständen nicht die gleiche Leistung erbringen. \footcite[Vgl.][S. 1]{lv2023parameter}
