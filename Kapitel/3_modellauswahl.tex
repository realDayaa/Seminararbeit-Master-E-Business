\section{Unterschiede zwischen aktuellen großen Sprachmodellen}

\subsection{Einführung in LLaMA und GPT-4}
LLaMA, entwickelt von Meta Platforms, ist eine Sammlung von Grundlagen-Sprachmodellen mit Parametern im Bereich von 7 Milliarden bis 65 Milliarden. 
LLaMA verwendet die Standard-Transformer-Architektur und setzt auf Techniken wie RMSNorm für Vor-Normalisierung, SwiGLU-Aktivierungsfunktion und rotierte Positions-Einbettungen (RoPE). 
Eine wichtige Verbesserung gegenüber dem Vorgängermodell ist die Erweiterung der Kontextlänge von 2048 auf 4096 und die Verwendung von Grouped Query Attention (GQA).